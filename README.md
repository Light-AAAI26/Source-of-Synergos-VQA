# Source-of-Synergos-VQA
This repository contains the reletive code for our AAAI-26 submission.
# ğŸŒ² Synergos-VQA ğŸŒ³

This is the official PyTorch implementation of the paper **"See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering"**. **Synergos-VQA** addresses the uni-dimensional evidence bottleneck in Knowledge-Based Visual Question Answering (KBVQA). We propose a synergistic reasoning framework that achieves a deeper and more robust visual understanding by concurrently generating and fusing three complementary evidence streams at inference time: **Holistic Evidence**, **Structural Evidence**, and **Causal Evidence**.
<img width="5063" height="2527" alt="Fig2" src="https://github.com/user-attachments/assets/8f96d0d6-fcaa-45f3-ae29-4bc035d7e37a" />



## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [âš ï¸ Hardware Requirements](#ï¸-hardware-requirements)
- [ğŸ“‚ Project Structure](#-project-structure)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸš€ End-to-End Reproduction Workflow](#-end-to-end-reproduction-workflow)
  - [Step 1: Download Datasets](#step-1-download-datasets)
  - [Step 2: Build the Prototype Library (Offline)](#step-2-build-the-prototype-library-offline)
  - [Step 3: Train the Full Model](#step-3-train-the-full-model)
  - [Step 4: Evaluate the Model](#step-4-evaluate-the-model)
- [ğŸ““ Demo Notebook](#-demo-notebook)
- [ğŸ¤ Contributing](#-contributing)


## âœ¨ Features

- ğŸ’¡ Full implementation of the **Synergos-VQA** framework, featuring its three core evidence-generation tracks.
- ğŸ¤– Plug-and-play modules for **Holistic Scene Analysis**, **Structural Backbone Reasoning**, and a **Causal Robustness Probe**.
- ğŸ”„ An efficient multi-source evidence fusion module based on **Fusion-in-Decoder (FiD) / T5**.
- ğŸ† Complete scripts to reproduce the state-of-the-art (SOTA) results on benchmarks like **OK-VQA**, **A-OKVQA**, and **ScienceQA**.
- ğŸ”Œ Demonstrates the framework's capability as a model-agnostic toolkit to enhance the reasoning of various MLLMs.

## âš ï¸ Hardware Requirements

> **Important:** Training and evaluating the full Synergos-VQA pipeline is computationally intensive due to the online evidence generation by large multimodal models.
> - **Recommended GPU:** NVIDIA A100 (80GB) or a comparable GPU with at least 48GB of VRAM.
> - **Recommended RAM:** At least 128GB of system RAM.
>
> *48GB is enough to train and evaluate the full Synergos-VQA, if your GPU has a VRAM lower than 40GB, please use the 3B MLLMs as the external engine*

## ğŸ“‚ Project Structure
```bash
ğŸ“‚ Synergos-VQA/
â”œâ”€â”€ configs/              # .yaml configuration files for experiments
â”œâ”€â”€ data/                 # Placeholder for datasets
â”œâ”€â”€ notebooks/            # Jupyter Notebooks for analysis and demos (demo.ipynb)
â”œâ”€â”€ pipelines/            # Offline pipeline scripts (e.g., build_prototype_library.py)
â”œâ”€â”€ scripts/              # Bash scripts for running full experiments
â”œâ”€â”€ src/                  # Core source code
â”‚   â”œâ”€â”€ evidence_generator/ # Implementation of the three evidence generators
â”‚   â”œâ”€â”€ fusion_module/      # Implementation of the FiD/T5 fusion module
â”‚   â”œâ”€â”€ data_loader.py    # Data loaders
â”‚   â””â”€â”€ utils.py          # Utility functions
â”œâ”€â”€ checkpoints/          # Directory for saving/loading model weights and prototype library
â”œâ”€â”€ train.py              # Main script for end-to-end training
â”œâ”€â”€ evaluate.py           # Main script for end-to-end evaluation
â”œâ”€â”€ requirements.yaml      # Project dependencies
â””â”€â”€ README.md             # This file
```
## âš™ï¸ Installation

1.  **Clone the repository:**

2.  **Create and activate a virtual environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    *Please ensure you have a compatible version of PyTorch and CUDA installed for your system.*
    ```bash
    pip install -r requirements.txt
    ```

## ğŸš€ End-to-End Reproduction Workflow

Follow these steps to reproduce the results from scratch.

### Step 1: Download Datasets

Please download the required datasets and place them in the `./data` directory. You will need:
- **OK-VQA**
- **A-OKVQA**
- **ScienceQA**
- **COCO (for Prototype Library)**

Your `./data` directory should look something like this:
data/
â”œâ”€â”€ okvqa/
â”‚   â”œâ”€â”€ train2014/
â”‚   â”œâ”€â”€ val2014/
â”‚   â””â”€â”€ ...
â””â”€â”€ coco/
â”œâ”€â”€ train2017/
â”œâ”€â”€ val2017/
â””â”€â”€ ...


### Step 2: Build the Prototype Library (Offline)

This step uses the COCO dataset to build the prototype library required for the Structural Backbone Reasoning module.

```bash
python3 pipelines/build_prototype_library.py \
    --corpus_path ./data/coco \
    --output_path ./checkpoints/prototype_library.pkl
```
This will generate the prototype_library.pkl file inside the checkpoints directory.

Step 3: Train the Full Model
This command launches the end-to-end training process. For each batch, it will perform online evidence generation before feeding the data to the decision module for training.

```Bash

# Example for training on OK-VQA
python3 train.py --config configs/synergos_okvqa_train.yaml
Training logs and model checkpoints will be saved to the directory specified in your config file.
```
Step 4: Evaluate the Model
4a. Evaluate Your Own Trained Model
Once your training is complete, use the following command to evaluate your model's performance on the test set.

```Bash

python3 evaluate.py \
    --config configs/synergos_okvqa_eval.yaml \
    --checkpoint /path/to/your/trained_model.pth
```

Download the used pre-trained model:

Link: [åœ¨æ­¤å¤„æ”¾ç½®æ‚¨çš„ Google Drive, Hugging Face, æˆ–å…¶ä»–å¯è®¿é—®çš„ä¸‹è½½é“¾æ¥]

Run evaluation:

```Bash

python3 evaluate.py \
    --config configs/synergos_okvqa_eval.yaml \
    --checkpoint ./checkpoints/synergos_vqa_sota.pth
This should reproduce the SOTA accuracy reported in our paper's main results table.
```
## ğŸ““ Demo Notebook
For an intuitive demonstration of the Synergos-VQA reasoning process, please see notebooks/demo.ipynb. This notebook loads our pre-trained models and provides a step-by-step visualization of how the Holistic, Structural, and Causal evidence streams are generated and fused to produce the final answer for a given image-question pair.


## ğŸ¤ Contributing
We welcome contributions to enhance this project. If you're interested in contributing, please follow these steps:

ğŸ´ Fork the repository

ğŸŒ¿ Create a new branch for your feature or bug fix (git checkout -b feature/AmazingFeature)

ğŸ’¬ Commit your changes with clear and concise messages (git commit -m 'Add some AmazingFeature')

ğŸ“¤ Push your branch to your forked repository (git push origin feature/AmazingFeature)

ğŸ“¥ Open a Pull Request detailing your changes and the motivation behind them.

Please ensure that your code adheres to the existing style and includes appropriate tests.
